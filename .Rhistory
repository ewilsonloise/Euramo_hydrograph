cat("Validate:", nrow(validate_data), "rows\n")
# 4. Define features and target variable  ----
categorical_features <- c("site")
target <- c("level_metres")
features <- setdiff(names(train_data), c(target, "quality_rainfall", "quality_level", "quality_discharge", "var"))  # Exclude 'quality' from features as well
print(features)
x_train <- train_data[, features]
y_train <- train_data[, target]
X_test <- test_data[, features]
y_test <- test_data[, target]
# 5. Pool data for Catboost  ----
train_pool <- catboost.load_pool(data = x_train, label = y_train)
# 5. Pool data for Catboost  ----
train_pool <- catboost.load_pool(data = x_train, label = y_train)
tibble(train_pool)
tibble(x_train)
sapply(x_train, class)
features <- setdiff(names(train_data), c(target, "quality_rainfall", "quality_level", "quality_discharge", "var", "time"))  # Exclude 'quality' from features as well
categorical_features <- c("site")
target <- c("level_metres")
features <- setdiff(names(train_data), c(target, "quality_rainfall", "quality_level", "quality_discharge", "var", "time"))  # Exclude 'quality' from features as well
print(features)
x_train$site <- as.factor(x_train$site)
x_train <- train_data[, features]
y_train <- train_data[, target]
X_test$site <- as.factor(X_test$site)
X_test <- test_data[, features]
y_test <- test_data[, target]
# 5. Pool data for Catboost  ----
train_pool <- catboost.load_pool(data = x_train, label = y_train)
test_pool <- catboost.load_pool(data = x_test, label = y_test)
X_test$site <- as.factor(X_test$site)
X_test <- test_data[, features]
y_test <- test_data[, target]
test_pool <- catboost.load_pool(data = x_test, label = y_test)
categorical_features <- c("site")
target <- c("level_metres")
# 2025 ----
# 1. Install + Load Packages ----
# library(devtools)
# library(remotes)
# library(renv)
# devtools::install_github('catboost/catboost', subdir = 'catboost/R-package')
library(catboost)
library(tidyverse)
library(rsample)
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
# Note that catboost can't handle date/time strings
dat$time <- as.POSIXct(dat$time, format = "%Y-%m-%d %H")
dat$year <- as.numeric(format(dat$time, "%Y"))
dat$month <- as.numeric(format(dat$time, "%m"))
dat$day <- as.numeric(format(dat$time, "%d"))
dat$hour <- as.numeric(format(dat$time, "%H"))
# 3. Subset data ----
set.seed(123)
# Create a random sample of row indices
n <- nrow(dat)
train_idx <- sample(1:n, size = 0.6 * n)  # 60% for training
remaining <- setdiff(1:n, train_idx)      # Remaining 40%
# Split remaining data into test (20%) and validation (20%)
# Note that this method was used to ensure that there is no repeated data being used
test_idx <- sample(remaining, size = 0.5 * length(remaining)) # Uses 50% of left over data, e.g., 20% of original data
validate_idx <- setdiff(remaining, test_idx)
# Create the subsets
train_data <- dat[train_idx, ]
test_data <- dat[test_idx, ]
validate_data <- dat[validate_idx, ]
# Check that it split up correctly
cat("Train:", nrow(train_data), "rows\n")
cat("Test:", nrow(test_data), "rows\n")
cat("Validate:", nrow(validate_data), "rows\n")
# 4. Define features and target variable  ----
categorical_features <- c("site")
target <- c("level_metres")
features <- setdiff(names(train_data), c(target, "quality_rainfall", "quality_level", "quality_discharge", "var", "time"))  # Exclude 'quality' from features as well
train_data$site <- as.factor(train_data$site)
test_data$site <- as.factor(test_data$site)
x_train <- train_data[, features]
y_train <- train_data[, target]
x_test <- test_data[, features]
y_test <- test_data[, target]
# 5. Pool data for Catboost  ----
train_pool <- catboost.load_pool(data = x_train, label = y_train)
test_pool <- catboost.load_pool(data = x_test, label = y_test)
# 6. Define Model Parameters  ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression
iterations = 1000,       # If overfitting = less iterations, if underfitting = more iterations
depth = 6,               # deeper trees can learn more complex patterns - arverage is 3-10, smaller = may prevent over fitting
learning_rate = 0.02,     # lower end of range is 0.01, 0.05 higher end is 0.1 to 0.3 - common range between 0.01 to 0.1
cat_features = categorical_features,
verbose = 100,
l2_leaf_reg = 10,         # L2 regularization parameter
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50  # Stop after 50 rounds without improvement# Show progress every 100 iterations
)
model <- catboost.train(train_pool, params = params)
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression
iterations = 1000,       # If overfitting = less iterations, if underfitting = more iterations
depth = 6,               # deeper trees can learn more complex patterns - arverage is 3-10, smaller = may prevent over fitting
learning_rate = 0.02,     # lower end of range is 0.01, 0.05 higher end is 0.1 to 0.3 - common range between 0.01 to 0.1
cat_features = "site",
verbose = 100,
l2_leaf_reg = 10,         # L2 regularization parameter
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50  # Stop after 50 rounds without improvement# Show progress every 100 iterations
)
model <- catboost.train(train_pool, params = params)
cat_features <- c("site")
# 9 Evaluate model performance ----
library(Metrics)
install.packages("Metrics")
# 9 Evaluate model performance ----
library(Metrics)
rmse(y_test, predictions)  # Root Mean Square Error
# 8 Make predictions ----
predictions <- catboost.predict(model, test_pool)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 8 Make predictions ----
predictions <- catboost.predict(model, test_pool)
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
# Note that catboost can't handle date/time strings
dat$time <- as.POSIXct(dat$time, format = "%Y-%m-%d %H")
dat$year <- as.numeric(format(dat$time, "%Y"))
dat$month <- as.numeric(format(dat$time, "%m"))
dat$day <- as.numeric(format(dat$time, "%d"))
dat$hour <- as.numeric(format(dat$time, "%H"))
# Define categorical features
cat_features <- c("site")
# Define target and feature variables
target <- c("level_metres")  # Target variable is 'level_metres'
# 5. Pool Data for CatBoost ----
train_pool <- catboost.load_pool(
data = x_train,
label = y_train,
cat_features = which(names(x_train) %in% cat_features)  # Ensure categorical features are recognized
)
# 4. Define features and target variable  ----
# Define categorical features
cat_features <- c("site")
# Define target and feature variables
target <- c("level_metres")  # Target variable is 'level_metres'
features <- setdiff(names(train_data), c(target, "quality_rainfall", "quality_level", "quality_discharge", "var", "time"))
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
# Note that catboost can't handle date/time strings
dat$time <- as.POSIXct(dat$time, format = "%Y-%m-%d %H")
dat$year <- as.numeric(format(dat$time, "%Y"))
dat$month <- as.numeric(format(dat$time, "%m"))
dat$day <- as.numeric(format(dat$time, "%d"))
dat$hour <- as.numeric(format(dat$time, "%H"))
# 3. Subset data ----
set.seed(123)
# Create a random sample of row indices
n <- nrow(dat)
train_idx <- sample(1:n, size = 0.6 * n)  # 60% for training
remaining <- setdiff(1:n, train_idx)      # Remaining 40%
# Split remaining data into test (20%) and validation (20%)
# Note that this method was used to ensure that there is no repeated data being used
test_idx <- sample(remaining, size = 0.5 * length(remaining)) # Uses 50% of left over data, e.g., 20% of original data
validate_idx <- setdiff(remaining, test_idx)
# Create the subsets
train_data <- dat[train_idx, ]
test_data <- dat[test_idx, ]
validate_data <- dat[validate_idx, ]
# Check that it split up correctly
cat("Train:", nrow(train_data), "rows\n")
cat("Test:", nrow(test_data), "rows\n")
cat("Validate:", nrow(validate_data), "rows\n")
# 4. Define features and target variable  ----
# Define categorical features
cat_features <- c("site")
# Define target and feature variables
target <- c("level_metres")  # Target variable is 'level_metres'
features <- setdiff(names(train_data), c(target, "quality_rainfall", "quality_level", "quality_discharge", "var", "time"))
# Exclude 'quality' columns and 'time' from features
# Convert categorical features to factor
train_data$site <- as.factor(train_data$site)
test_data$site <- as.factor(test_data$site)
# Print the list of features used
print(features)
# Split into X (features) and Y (target)
x_train <- train_data[, features]  # Feature variables for training set
y_train <- train_data[, target]    # Target variable for training set
x_test <- test_data[, features]    # Feature variables for test set
y_test <- test_data[, target]      # Target variable for test set
train_pool <- catboost.load_pool(
data = x_train,
label = y_train,
cat_features = which(names(x_train) %in% cat_features)  # Ensure categorical features are recognized
)
# 4. Define features and target variable  ----
# Define target and feature variables
target <- c("level_metres")  # Target variable is 'level_metres'
features <- setdiff(names(train_data), c(target, "quality_rainfall", "quality_level", "quality_discharge", "var", "time"))
# Exclude 'quality' columns and 'time' from features
# Convert categorical features to factor
train_data$site <- as.factor(train_data$site)
test_data$site <- as.factor(test_data$site)
# Print the list of features used
print(features)
# Split into X (features) and Y (target)
x_train <- train_data[, features]  # Feature variables for training set
y_train <- train_data[, target]    # Target variable for training set
x_test <- test_data[, features]    # Feature variables for test set
y_test <- test_data[, target]      # Target variable for test set
# 5. Pool Data for CatBoost ----
train_pool <- catboost.load_pool(
data = x_train,
label = y_train,
cat_features = which(names(x_train) %in% cat_features)  # Ensure categorical features are recognized
)
# 5. Pool Data for CatBoost ----
train_pool <- catboost.load_pool(
data = x_train,
label = y_train,
cat_features = "site")  # Ensure categorical features are recognized
# 5. Pool Data for CatBoost ----
train_pool <- catboost.load_pool(
data = x_train,
label = y_train)  # Ensure categorical features are recognized
test_pool <- catboost.load_pool(
data = x_test,
label = y_test)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
cat_features = which(names(x_train) %in% cat_features),  # Ensure categorical feature index is correctly passed
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50  # Stop training after 50 rounds without improvement
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50  # Stop training after 50 rounds without improvement
)
tibble(train_pool)
tibble(x_train)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
cat_features = TRUE,  # Ensure categorical feature index is correctly passed
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50  # Stop training after 50 rounds without improvement
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 8 Make predictions ----
predictions <- catboost.predict(model, test_pool)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
?? cat_features
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50  # Stop training after 50 rounds without improvement
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
sum(is.na(y_train))  # Count NA values in the target
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSEWithUncertainty",  # "RMSE" being used for regression and for being able to keep the NA values
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50  # Stop training after 50 rounds without improvement
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
vis_miss(x_train)
vis_miss(x_train, warn_large_data = FALSE)
vis_miss(dat, warn_large_data = FALSE)
test <- dat[!is.na(dat), ]
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
test <- dat[!is.na(dat), ]
test <- dat[complete.cases(dat), ]
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
test <- na.omit(dat)
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
mydat_tibble_no_NA <- dat %>%
drop_na()
View(test)
View(dat)
tibble(dat)
mydat_tibble_no_NA <- dat %>%
drop_na("quality_discharge", "level_metres", "discharge_cumecs")
?? nan.mode
?? nan_mode
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSEWithUncertainty",  # "RMSE" being used for regression and for being able to keep the NA values
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50,
nan_mode = 'Forbidden' # Stop training after 50 rounds without improvement
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression and for being able to keep the NA values
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50,
missing_value_mode = "Forbidden"  # Missing values are not allowed and will result in an error
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression and for being able to keep the NA values
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50,
nan_mode = "Forbidden"   # Missing values are not allowed and will result in an error
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression and for being able to keep the NA values
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50,
nan_mode = "Max"   # Missing values are not allowed and will result in an error
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSEWithUncertainty",  # "RMSE" being used for regression and for being able to keep the NA values
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50,
nan_mode = "Max"   # Missing values are not allowed and will result in an error
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
dat[is.na] <- NaN
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
dat[] <- lapply(dat, function(x) {
if(is.numeric(x)) {
# Replace NA with NaN for numeric columns
x[is.na(x)] <- NaN
}
return(x)
})
summary(dat)  # Check if NA values have been replaced with NaN
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
dat[] <- lapply(dat, function(x) {
if(is.numeric(x)) {
# Replace NA with NaN for numeric columns
x[is.na(x)] <- '-9999'
}
return(x)
})
summary(dat)  # Check if NA values have been replaced with NaN
# Note that catboost can't handle date/time strings
dat$time <- as.POSIXct(dat$time, format = "%Y-%m-%d %H")
dat$year <- as.numeric(format(dat$time, "%Y"))
dat$month <- as.numeric(format(dat$time, "%m"))
dat$day <- as.numeric(format(dat$time, "%d"))
dat$hour <- as.numeric(format(dat$time, "%H"))
# 3. Subset data ----
set.seed(123)
# Create a random sample of row indices
n <- nrow(dat)
train_idx <- sample(1:n, size = 0.6 * n)  # 60% for training
remaining <- setdiff(1:n, train_idx)      # Remaining 40%
# Split remaining data into test (20%) and validation (20%)
# Note that this method was used to ensure that there is no repeated data being used
test_idx <- sample(remaining, size = 0.5 * length(remaining)) # Uses 50% of left over data, e.g., 20% of original data
validate_idx <- setdiff(remaining, test_idx)
# Create the subsets
train_data <- dat[train_idx, ]
test_data <- dat[test_idx, ]
validate_data <- dat[validate_idx, ]
# Check that it split up correctly
cat("Train:", nrow(train_data), "rows\n")
cat("Test:", nrow(test_data), "rows\n")
cat("Validate:", nrow(validate_data), "rows\n")
# Define target and feature variables
target <- c("level_metres")  # Target variable is 'level_metres'
features <- setdiff(names(train_data), c(target, "quality_rainfall", "quality_level", "quality_discharge", "var", "time"))
# Convert categorical features to factor
train_data$site <- as.factor(train_data$site)
test_data$site <- as.factor(test_data$site)
# Print the list of features used
print(features)
# Split into X (features) and Y (target)
x_train <- train_data[, features]  # Feature variables for training set
y_train <- train_data[, target]    # Target variable for training set
x_test <- test_data[, features]    # Feature variables for test set
y_test <- test_data[, target]      # Target variable for test set
# 5. Pool Data for CatBoost ----
train_pool <- catboost.load_pool(
data = x_train,
label = y_train)
# 5. Pool Data for CatBoost ----
train_pool <- catboost.load_pool(
data = x_train,
label = y_train)
test_pool <- catboost.load_pool(
data = x_test,
label = y_test)
# 5. Pool Data for CatBoost ----
train_pool <- catboost.load_pool(
data = x_train,
label = y_train)
test_pool <- catboost.load_pool(
data = x_test,
label = y_test)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression and for being able to keep the NA values
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50,
# nan_mode = "Forbidden"   # Missing values are not allowed and will result in an error
)
# 6. Define Model Parameters ----
params <- list(
loss_function = "RMSE",  # "RMSE" being used for regression and for being able to keep the NA values
iterations = 1000,       # If overfitting = reduce iterations, if underfitting = increase iterations
depth = 6,               # Tree depth: common range 3-10, smaller values can prevent overfitting
learning_rate = 0.02,     # Lower range: 0.01 - 0.05, higher range: 0.1 - 0.3
verbose = 100,           # Show progress every 100 iterations
l2_leaf_reg = 10,        # L2 regularization parameter (higher values prevent overfitting)
random_seed = 42,        # Set random seed for reproducibility
early_stopping_rounds = 50
# nan_mode = "Forbidden"   # Missing values are not allowed and will result in an error
)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 7. Train the Model ----
model <- catboost.train(train_pool, params = params)
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
skimr::skim(dat)
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
# 2. Prep data ----
dat <- read_csv("Data/dat.csv")
library(tidyverse)
# 1. Install + Load Packages ----
# library(devtools)
# library(remotes)
# library(renv)
# devtools::install_github('catboost/catboost', subdir = 'catboost/R-package')
library(catboost)
library(rsample)
